{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNumKN2Y8RTv7a2hS9swtck"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"lead_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "VGTs-it6rTT6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS ===\n",
        "import requests\n",
        "from requests.auth import HTTPBasicAuth\n",
        "from transformers import pipeline\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === API KEYS ===\n",
        "SERPER_API_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "HUNTER_API_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "DATAFORSEO_USERNAME = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "DATAFORSEO_PASSWORD = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "# === Load Hugging Face QA Model ===\n",
        "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "# === Helper Functions ===\n",
        "\n",
        "def get_website_from_serper(company_name):\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    try:\n",
        "        res = requests.post(url, headers=headers, json={\"q\": company_name})\n",
        "        data = res.json()\n",
        "        if 'knowledgeGraph' in data and 'website' in data['knowledgeGraph']:\n",
        "            return data['knowledgeGraph']['website']\n",
        "        for result in data.get('organic', []):\n",
        "            if 'link' in result:\n",
        "                return result['link']\n",
        "    except Exception as e:\n",
        "        print(\"Serper error:\", e)\n",
        "    return None\n",
        "\n",
        "def get_email_from_hunter(domain):\n",
        "    url = f\"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={HUNTER_API_KEY}\"\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        data = res.json()\n",
        "        return [e['value'] for e in data.get(\"data\", {}).get(\"emails\", [])]\n",
        "    except Exception as e:\n",
        "        print(\"Hunter error:\", e)\n",
        "    return []\n",
        "\n",
        "def extract_emails_from_website(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return list(set(re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", soup.get_text())))\n",
        "    except Exception as e:\n",
        "        print(\"Scraping error:\", e)\n",
        "        return []\n",
        "\n",
        "def get_ceo_from_dataforseo(company_name):\n",
        "    url = \"https://api.dataforseo.com/v3/serp/google/organic/live/advanced\"\n",
        "    payload = [{\n",
        "        \"keyword\": f\"{company_name} CEO\",\n",
        "        \"location_name\": \"United States\",\n",
        "        \"language_code\": \"en\",\n",
        "        \"device\": \"desktop\"\n",
        "    }]\n",
        "    full_name_pattern = re.compile(r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\")\n",
        "\n",
        "    try:\n",
        "        res = requests.post(url, auth=HTTPBasicAuth(DATAFORSEO_USERNAME, DATAFORSEO_PASSWORD), json=payload)\n",
        "        data = res.json()\n",
        "        items = data['tasks'][0]['result'][0]['items']\n",
        "        best_result = None\n",
        "        for item in items:\n",
        "            text = item.get(\"description\", \"\") or item.get(\"title\", \"\")\n",
        "            if \"CEO\" in text and company_name.lower() in text.lower():\n",
        "                result = qa_model(question=f\"Who is the CEO of {company_name}?\", context=text)\n",
        "                if result['score'] > 0.75 and full_name_pattern.match(result['answer']):\n",
        "                    return result['answer']\n",
        "                if not best_result or result['score'] > best_result['score']:\n",
        "                    best_result = result\n",
        "        if best_result and full_name_pattern.match(best_result['answer']):\n",
        "            return best_result['answer']\n",
        "    except Exception as e:\n",
        "        print(\"DataForSEO CEO error:\", e)\n",
        "    return \"Not Found\"\n",
        "\n",
        "def get_linkedin_from_dataforseo(company_name):\n",
        "    url = \"https://api.dataforseo.com/v3/serp/google/organic/live/advanced\"\n",
        "    payload = [{\n",
        "        \"keyword\": f\"{company_name} LinkedIn\",\n",
        "        \"location_name\": \"United States\",\n",
        "        \"language_code\": \"en\",\n",
        "        \"device\": \"desktop\"\n",
        "    }]\n",
        "    try:\n",
        "        res = requests.post(url, auth=HTTPBasicAuth(DATAFORSEO_USERNAME, DATAFORSEO_PASSWORD), json=payload)\n",
        "        data = res.json()\n",
        "        items = data['tasks'][0]['result'][0]['items']\n",
        "        for item in items:\n",
        "            if \"linkedin.com/company\" in item.get(\"url\", \"\"):\n",
        "                return item[\"url\"]\n",
        "    except Exception as e:\n",
        "        print(\"LinkedIn scrape error:\", e)\n",
        "    return \"Not Found\"\n",
        "\n",
        "def get_revenue_and_employees(company_name, username, password):\n",
        "    url = \"https://api.dataforseo.com/v3/serp/google/organic/live/advanced\"\n",
        "    payload = [{\n",
        "        \"keyword\": f\"{company_name} revenue and employees\",\n",
        "        \"location_name\": \"United States\",\n",
        "        \"language_code\": \"en\",\n",
        "        \"device\": \"desktop\"\n",
        "    }]\n",
        "\n",
        "    revenue_pattern = re.compile(r\"\\$[0-9,.]+\\s*(billion|million|B|M)?\", re.IGNORECASE)\n",
        "    employee_pattern = re.compile(\n",
        "        r\"\\b(?:over\\s*)?([0-9,.]+(?:\\s*[kKmMbB])?)\\s*(?:employees|people|staff|personnel)\\b\", re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        res = requests.post(url, auth=HTTPBasicAuth(username, password), json=payload)\n",
        "        data = res.json()\n",
        "        items = data['tasks'][0]['result'][0]['items']\n",
        "\n",
        "        revenue = None\n",
        "        employee_counts = []\n",
        "\n",
        "        for item in items:\n",
        "            text = item.get(\"description\", \"\") or item.get(\"title\", \"\")\n",
        "            if not revenue:\n",
        "                rev_match = revenue_pattern.search(text)\n",
        "                if rev_match:\n",
        "                    revenue = rev_match.group()\n",
        "\n",
        "            emp_matches = employee_pattern.findall(text)\n",
        "            for emp_raw in emp_matches:\n",
        "                emp_clean = emp_raw.replace(\",\", \"\").strip().lower()\n",
        "                try:\n",
        "                    if 'k' in emp_clean:\n",
        "                        count = float(emp_clean.replace('k', '')) * 1_000\n",
        "                    elif 'm' in emp_clean:\n",
        "                        count = float(emp_clean.replace('m', '')) * 1_000_000\n",
        "                    elif 'b' in emp_clean:\n",
        "                        count = float(emp_clean.replace('b', '')) * 1_000_000_000\n",
        "                    else:\n",
        "                        count = float(emp_clean)\n",
        "\n",
        "                    if 1_000 <= count <= 1_000_000:\n",
        "                        employee_counts.append(int(count))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if employee_counts:\n",
        "            best_emp_estimate = max(set(employee_counts), key=employee_counts.count) if len(set(employee_counts)) < len(employee_counts) else max(employee_counts)\n",
        "            employees = f\"~{int(best_emp_estimate):,} employees\"\n",
        "        else:\n",
        "            employees = \"Approximate size not found\"\n",
        "\n",
        "        return revenue or \"Not Found\", employees\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Revenue/Employees extract error:\", e)\n",
        "        return \"Not Found\", \"Not Found\"\n",
        "\n",
        "# === FINAL: Lead Generator ===\n",
        "def get_lead_info(company_name):\n",
        "    print(f\"\\nðŸ” Processing: {company_name}\")\n",
        "    website = get_website_from_serper(company_name)\n",
        "    print(\"ðŸŒ Website:\", website)\n",
        "\n",
        "    domain = urlparse(website).netloc if website else None\n",
        "    emails = get_email_from_hunter(domain) if domain else []\n",
        "    if not emails and website:\n",
        "        emails = extract_emails_from_website(website)\n",
        "    print(\"ðŸ“§ Emails:\", emails)\n",
        "\n",
        "    ceo_name = get_ceo_from_dataforseo(company_name)\n",
        "    print(\"ðŸ‘¤ CEO:\", ceo_name)\n",
        "\n",
        "    linkedin = get_linkedin_from_dataforseo(company_name)\n",
        "    print(\"ðŸ”— LinkedIn:\", linkedin)\n",
        "\n",
        "    revenue, employees = get_revenue_and_employees(company_name, DATAFORSEO_USERNAME, DATAFORSEO_PASSWORD)\n",
        "    print(\"ðŸ’° Revenue:\", revenue)\n",
        "    print(\"ðŸ‘¥ Employees:\", employees)\n",
        "\n",
        "    return {\n",
        "        \"company\": company_name,\n",
        "        \"website\": website,\n",
        "        \"emails\": emails,\n",
        "        \"ceo_name\": ceo_name,\n",
        "        \"linkedin_url\": linkedin,\n",
        "        \"revenue\": revenue,\n",
        "        \"employees\": employees\n",
        "    }\n",
        "\n",
        "# === Optional: Batch Processing with Retry & Delay ===\n",
        "def process_companies(companies):\n",
        "    all_results = []\n",
        "    for company in companies:\n",
        "        result = get_lead_info(company)\n",
        "\n",
        "        # Retry revenue/employees if not found\n",
        "        if result[\"employees\"] == \"Approximate size not found\" or result[\"employees\"] == \"Not Found\":\n",
        "            print(\"ðŸ” Retrying employee data...\")\n",
        "            result[\"revenue\"], result[\"employees\"] = get_revenue_and_employees(company, DATAFORSEO_USERNAME, DATAFORSEO_PASSWORD)\n",
        "\n",
        "        all_results.append(result)\n",
        "        time.sleep(2)  # prevent API overload\n",
        "    return all_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BezofTnh-H6N",
        "outputId": "2b993ffd-7a72-403f-dab7-adfebafc6b0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "companies = [\"Apple\", \"Microsoft\",\"Zomato\"]\n",
        "results = process_companies(companies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gp-sVcm-_Pu",
        "outputId": "d4eef1b6-d5cd-420c-88f4-c45c61867fdf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ” Processing: Apple\n",
            "ðŸŒ Website: https://www.apple.com/\n",
            "ðŸ“§ Emails: ['rayna_schwartz@apple.com', 'missy_pool@apple.com', 'ashish.sharma@apple.com', 'hkrishnamurthy@apple.com', 'fkhan@apple.com', 'aminah_charles@apple.com', 'vishesh_yadav@apple.com', 'xiaoge_su@apple.com', 'anna_m@apple.com', 'lpan@apple.com']\n",
            "ðŸ‘¤ CEO: Tim Cook\n",
            "ðŸ”— LinkedIn: https://www.linkedin.com/company/apple\n",
            "ðŸ’° Revenue: $391.04 billion\n",
            "ðŸ‘¥ Employees: ~164,000 employees\n",
            "\n",
            "ðŸ” Processing: Microsoft\n",
            "ðŸŒ Website: https://www.microsoft.com/\n",
            "ðŸ“§ Emails: ['vinod.kumar@microsoft.com', 'carlos.fernandez@microsoft.com', 'pamela.almaguer@microsoft.com', 'rolf.harms@microsoft.com', 'patrick.larkin@microsoft.com', 'denise.begley@microsoft.com', 'mark.jacobson@microsoft.com', 'richard.rundle@microsoft.com', 'tomsmith@microsoft.com', 'lianw@microsoft.com']\n",
            "ðŸ‘¤ CEO: Satya Narayana Nadella\n",
            "ðŸ”— LinkedIn: https://www.linkedin.com/company/microsoft\n",
            "ðŸ’° Revenue: $40.9 billion\n",
            "ðŸ‘¥ Employees: ~228,000 employees\n",
            "\n",
            "ðŸ” Processing: Zomato\n",
            "ðŸŒ Website: https://www.zomato.com/\n",
            "ðŸ“§ Emails: ['manoj@zomato.com', 'munnaz@zomato.com', 'dharmendra@zomato.com', 'alan@zomato.com', 'kailash@zomato.com', 'palak@zomato.com', 'raman@zomato.com', 'ashwini@zomato.com', 'rajat@zomato.com', 'girish@zomato.com']\n",
            "ðŸ‘¤ CEO: Deepinder Goyal\n",
            "ðŸ”— LinkedIn: https://in.linkedin.com/company/zomato\n",
            "ðŸ’° Revenue: $62.5M\n",
            "ðŸ‘¥ Employees: ~14,294 employees\n"
          ]
        }
      ]
    }
  ]
}